# Run RAGAS Evaluation with Langfuse Dashboard

## Step-by-Step Guide

### Step 1: Add Your API Keys
Edit the `.env` file in your project root and replace the placeholder keys:

```bash
# OpenAI API Configuration
OPENAI_API_KEY=your_actual_openai_key_here

# Langfuse Configuration (replace with your actual keys)
LANGFUSE_PUBLIC_KEY="pk_lf_your_actual_public_key_here"
LANGFUSE_SECRET_KEY="sk_lf_your_actual_secret_key_here"
LANGFUSE_HOST="https://cloud.langfuse.com"
```

### Step 2: Test Langfuse Connection
```bash
python test_langfuse_connection.py
```

You should see:
```
âœ… API keys found
âœ… Test trace created
âœ… Test generation created
âœ… Langfuse tracker is enabled
ðŸŽ‰ SUCCESS! Langfuse is working correctly.
```

### Step 3: Run RAGAS Evaluation with Langfuse Tracking
```bash
python src/eval_ragas_langfuse.py
```

This will:
- âœ… Run 7 comprehensive test queries
- âœ… Calculate RAGAS metrics (faithfulness, relevancy, etc.)
- âœ… Send all data to your Langfuse dashboard
- âœ… Save detailed results locally

Expected output:
```
ðŸ§® Running RAGAS evaluation (this may take a few minutes)...
ðŸŽ¯ RAGAS EVALUATION RESULTS WITH LANGFUSE TRACKING
================================================================
ðŸ“Š Total Queries: 7
âœ… Successful Retrievals: 5-7
ðŸ“„ Avg Documents Retrieved: 4-6

ðŸ§® RAGAS METRICS:
ðŸŸ¢ faithfulness: 0.750+ - Factual accuracy
ðŸŸ¢ answer_relevancy: 0.650+ - Answer relevance  
ðŸŸ¡ context_precision: 0.600+ - Context precision
...
```

### Step 4: View Results on Langfuse Dashboard

1. **Go to your Langfuse dashboard**: https://cloud.langfuse.com
2. **Select your project** (e.g., "UN Reports RAG")
3. **Look for traces** with session ID like `ragas_eval_20250901_123456`

**What you'll see:**
- ðŸ” **Individual traces** for each evaluation query
- ðŸ“Š **RAGAS metrics** as scores on each trace
- â±ï¸ **Response times** and token usage
- ðŸŽ¯ **Search performance** with similarity scores
- ðŸ“ **Full conversation context** (question â†’ search â†’ generation)

### Step 5: Analyze Your Results

**In Langfuse Dashboard:**
- **Traces tab**: See individual query executions
- **Sessions tab**: View the full evaluation session
- **Scores tab**: RAGAS metrics breakdown
- **Analytics**: Performance trends over time

**Key Metrics to Watch:**
- ðŸŽ¯ **Faithfulness > 0.7**: Low hallucination risk
- ðŸŽ¯ **Answer Relevancy > 0.6**: Good question matching
- ðŸŽ¯ **Context Precision > 0.5**: Relevant document retrieval

### Troubleshooting

**If connection test fails:**
1. Double-check your API keys in `.env`
2. Ensure no extra spaces around the keys
3. Verify you're using the correct Langfuse project

**If RAGAS evaluation is slow:**
- It processes 7 complex queries with OpenAI API calls
- Expected time: 3-5 minutes
- Each query involves: embedding â†’ search â†’ GPT-4o-mini generation

**If you see low scores:**
- Faithfulness < 0.6: Check for hallucinations
- Relevancy < 0.5: Questions may not match your corpus  
- Precision < 0.4: Consider adjusting search thresholds

### Next Steps

Once evaluation completes successfully:
1. âœ… **View results** in Langfuse dashboard
2. âœ… **Analyze RAGAS metrics** for system performance
3. âœ… **Set up continuous monitoring** for production use
4. âœ… **A/B test** different models or prompts

Your UN Reports RAG system is now fully monitored and evaluated! ðŸŽ‰